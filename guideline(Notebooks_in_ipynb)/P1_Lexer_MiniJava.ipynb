{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uehJXhC2mFw"
      },
      "source": [
        "# First Project: Lexer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dX24ex_C2mF0"
      },
      "source": [
        "The first project requires you to implement a scanner for the  language,\n",
        "specified by [MiniJava BNF Grammar](https://colab.research.google.com/drive/1kOGQxBlfoauANIgHYKTuDwhrpd7peea6?usp=sharing) notebook. Study the specification\n",
        "of MiniJava grammar carefully. To complete this first project, you will use the\n",
        "[SLY](https://sly.readthedocs.io/en/latest/sly.html), a Python version of the\n",
        "[lex/yacc](http://dinosaur.compilertools.net/) toolset with same functionality\n",
        "but with a friendlier interface. Please read the complete contents of this section and carefully complete the steps indicated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACFa_WUi2mF3"
      },
      "source": [
        "## Regular Expressions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nmo2tJRs2mF5"
      },
      "source": [
        "Regular expressions are concise ways of describing a set of strings that meet a given\n",
        "pattern. For example, we can specify the regular expression:\n",
        "```python\n",
        "r'[a-zA-Z_][0-9a-zA-Z_]*'\n",
        "```\n",
        "to describe valid identifiers in the MiniJava language. Regular expressions are a mini-language\n",
        "that lets you specify the rules for constructing a string set. This specification\n",
        "mini-language is very similar between the different programming languages that contain\n",
        "the concept of regular expressions (also called RE or REGEX). Thus, learning to write\n",
        "regular expressions in Python will also be useful for describing REs in other programming\n",
        "languages."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can use this Python code as a model to test your regular expressions:\n",
        "\n",
        "```python\n",
        "import re\n",
        "# regular expression\n",
        "identifier = r'[a-zA-Z_][0-9a-zA-Z_]*'\n",
        "# test identifier _123\n",
        "b = re.match(identifier, \"_123\")\n",
        "if b:\n",
        "    print(\"Identifier matches.\")\n",
        "else:\n",
        "    print(\"Error.\")\n",
        "```"
      ],
      "metadata": {
        "id": "-rSvfuHmjVV6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dmvb-E-02mGL"
      },
      "source": [
        "## Writing a Lexer\n",
        "The process of “lexing” is that of taking input text and breaking it down into a stream\n",
        "of tokens. Each token is like a valid word from the dictionary. Essentially, the role of\n",
        "the lexer is to simply make sure that the input text consists of valid symbols and tokens\n",
        "prior to any further processing related to parsing.\n",
        "\n",
        "Each token is defined by a regular expression. Thus, your task here is to define a set of\n",
        "regular expressions for the MiniJava language. The actual job of lexing will be handled by SLY.\n",
        "For a better understanding study the [Lex](https://sly.readthedocs.io/en/latest/sly.html#writing-a-lexer)\n",
        "section in the SLY documentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBMdDaoc2mGM"
      },
      "source": [
        "### Specification\n",
        "Your lexer must recognize the symbols and tokens of MiniJava Grammar. For instance, in the\n",
        "example below, the name on the left is the token name, and the value on the right is\n",
        "the matching text:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFaYTFic2mGM"
      },
      "source": [
        "Reserved Keywords:\n",
        "```\n",
        "    FOR   : 'for'\n",
        "    IF    : 'if'\n",
        "    PRINT : 'print'\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrsJd97H2mGN"
      },
      "source": [
        "Identifiers:\n",
        "```\n",
        "    ID    : any text starting with a letter or '_', followed by any number of letters,\n",
        "            digits, or underscores, that is not a reserved word.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jk1NcM8P2mGO"
      },
      "source": [
        "Some Operators and Delimiters:\n",
        "```\n",
        "    PLUS    : '+'\n",
        "    MINUS   : '-'\n",
        "    TIMES   : '*'\n",
        "    DIVIDE  : '/'\n",
        "    ASSIGN  : '='\n",
        "    SEMI    : ';'\n",
        "    LPAREN  : '('\n",
        "    RPAREN  : ')'\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrFxdB_u2mGP"
      },
      "source": [
        "Literals:\n",
        "```\n",
        "    INT_LITERAL : 123\n",
        "    CHAR_LITERAL : 'a'\n",
        "    STRING_LITERAL : \"Hello World\\n\"\n",
        "```\n",
        "\n",
        "For `INT_LITERAL`, you should only consider decimal numbers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRvblzGW2mGP"
      },
      "source": [
        "Comments:  To be ignored by your lexer\n",
        "```\n",
        "     //             Skips the rest of the line\n",
        "     /* ... */      Skips a block (no nesting allowed)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHBq69SO2mGR"
      },
      "source": [
        "### Lex Skeleton"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "import argparse\n",
        "import pathlib\n",
        "import sys\n",
        "\n",
        "from sly import Lexer\n",
        "\n",
        "\n",
        "class MJLexer(Lexer):\n",
        "    \"\"\"A lexer for the MiniJava language. After building it, set the\n",
        "    input text with input(), and call token() to get new\n",
        "    tokens.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, error_func):\n",
        "        \"\"\"Create a new Lexer.\n",
        "        An error function. Will be called with an error\n",
        "        message, line and column as arguments, in case of\n",
        "        an error during lexing.\n",
        "        \"\"\"\n",
        "        self.error_func = error_func\n",
        "        self.filename = \"\"\n",
        "\n",
        "        # Keeps track of the last token returned from self.token()\n",
        "        self.last_token = None\n",
        "\n",
        "    def _error(self, msg, token):\n",
        "        location = self._make_tok_location(token)\n",
        "        self.error_func(msg, location[0], location[1])\n",
        "        self.index += 1\n",
        "\n",
        "    def find_tok_column(self, token):\n",
        "        \"\"\"Find the column of the token in its line.\"\"\"\n",
        "        last_cr = self.text.rfind(\"\\n\", 0, token.index)\n",
        "        return token.index - last_cr\n",
        "\n",
        "    def _make_tok_location(self, token):\n",
        "        return (self.lineno, self.find_tok_column(token))\n",
        "\n",
        "    # Error handling\n",
        "    def error(self, t):\n",
        "        msg = f\"Illegal character {t.value[0]!r}\"\n",
        "        self._error(msg, t)\n",
        "\n",
        "    # Scanner (used only for test)\n",
        "    def scan(self, data):\n",
        "        output = \"\"\n",
        "        for token in self.tokenize(data):\n",
        "            token = (\n",
        "                f\"LexToken({token.type},'{token.value}',{token.lineno},{token.index})\"\n",
        "            )\n",
        "            print(token)\n",
        "            output += token + \"\\n\"\n",
        "        return output\n",
        "\n",
        "    # Set of token names.\n",
        "    tokens = {\n",
        "        # Reserved Keywords\n",
        "        \"CLASS\",\n",
        "        \"EXTENDS\",\n",
        "        \"PUBLIC\",\n",
        "        \"STATIC\",\n",
        "        \"VOID\",\n",
        "        \"MAIN\",\n",
        "        \"STRING\",\n",
        "        \"BOOLEAN\",\n",
        "        \"CHAR\",\n",
        "        \"INT\",\n",
        "        \"IF\",\n",
        "        \"ELSE\",\n",
        "        \"WHILE\",\n",
        "        \"FOR\",\n",
        "        \"ASSERT\",\n",
        "        \"BREAK\",\n",
        "        \"RETURN\",\n",
        "        \"NEW\",\n",
        "        \"THIS\",\n",
        "        \"TRUE\",\n",
        "        \"FALSE\",\n",
        "        \"LENGTH\",\n",
        "        \"PRINT\",\n",
        "        # Literals\n",
        "        \"ID\",\n",
        "        \"INT_LITERAL\",\n",
        "        \"CHAR_LITERAL\",\n",
        "        \"STRING_LITERAL\",\n",
        "        # Operators\n",
        "        \"EQ\",\n",
        "        \"NE\",\n",
        "        \"LE\",\n",
        "        \"GE\",\n",
        "        \"AND\",\n",
        "        \"OR\",\n",
        "        \"ASSIGN\",\n",
        "        \"LT\",\n",
        "        \"GT\",\n",
        "        \"PLUS\",\n",
        "        \"MINUS\",\n",
        "        \"TIMES\",\n",
        "        \"DIVIDE\",\n",
        "        \"MOD\",\n",
        "        \"NOT\",\n",
        "        # Punctuation\n",
        "        \"DOT\",\n",
        "        \"SEMI\",\n",
        "        \"COMMA\",\n",
        "        \"LPAREN\",\n",
        "        \"RPAREN\",\n",
        "        \"LBRACKET\",\n",
        "        \"RBRACKET\",\n",
        "        \"LBRACE\",\n",
        "        \"RBRACE\",\n",
        "    }\n",
        "\n",
        "    # ----------------------------------------------------------------\n",
        "    # Identifiers and reserved words\n",
        "    # ----------------------------------------------------------------\n",
        "    # A dictionary mapping reserved words to token types.\n",
        "    keywords = {\n",
        "        \"class\": \"CLASS\",\n",
        "        \"extends\": \"EXTENDS\",\n",
        "        \"public\": \"PUBLIC\",\n",
        "        \"static\": \"STATIC\",\n",
        "        \"void\": \"VOID\",\n",
        "        \"main\": \"MAIN\",\n",
        "        \"String\": \"STRING\",\n",
        "        \"boolean\": \"BOOLEAN\",\n",
        "        \"char\": \"CHAR\",\n",
        "        \"int\": \"INT\",\n",
        "        \"if\": \"IF\",\n",
        "        \"else\": \"ELSE\",\n",
        "        \"while\": \"WHILE\",\n",
        "        \"for\": \"FOR\",\n",
        "        \"assert\": \"ASSERT\",\n",
        "        \"break\": \"BREAK\",\n",
        "        \"return\": \"RETURN\",\n",
        "        \"new\": \"NEW\",\n",
        "        \"this\": \"THIS\",\n",
        "        \"true\": \"TRUE\",\n",
        "        \"false\": \"FALSE\",\n",
        "        \"length\": \"LENGTH\",\n",
        "    }\n",
        "\n",
        "    \n",
        "\n",
        "    # ----------------------------------------------------------------\n",
        "    # Rules\n",
        "    # ----------------------------------------------------------------\n",
        "    \n",
        "    # String containing ignored characters (spaces and tabs)\n",
        "    ignore = \" \\t\"\n",
        "\n",
        "    # Newlines\n",
        "    @_(r\"<Include a regex here for newline>\")\n",
        "    def ignore_newline(self, t):\n",
        "        self.lineno += len(t.value)\n",
        "\n",
        "    # Comments\n",
        "    @_(r\"<Include a regex here for comments>\")\n",
        "    def ignore_comment(self, t):\n",
        "        self.lineno += t.value.count('\\n')\n",
        "\n",
        "    # Identifiers and keywords\n",
        "    @_(r\"<Include a regex here for ID>\")\n",
        "    def ID(self, t):\n",
        "        t.type = self.keywords.get(t.value, \"ID\")\n",
        "        return t\n",
        "    \n",
        "    # ----------------------------------------------------------------\n",
        "    # Operators and punctuation (order matters: longer tokens first)\n",
        "    # ----------------------------------------------------------------\n",
        "    PLUS = r\"\\+\"   # Regex special characters must be escaped\n",
        "    MINUS = r\"-\"\n",
        "\n",
        "    # Continue the Lexer Rules\n",
        "    # ...\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "7vSNgubNoXLm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_-Bs1iR2mGS"
      },
      "source": [
        "## Testing\n",
        "For initial development, try running the lexer on a sample input file such as:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Hcp1WhDS2mGT"
      },
      "source": [
        "```java\n",
        "/* comment */\n",
        "class VariableTest {\n",
        "    int j = 3;\n",
        "    public static void main(String[] args) {\n",
        "        int i = this.j;\n",
        "        int k = 3;\n",
        "        int p = 2 * this.j;\n",
        "\n",
        "        assert p == 2 * i;\n",
        "    }\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qr0CzYJQ2mGT"
      },
      "source": [
        "And the result will look similar to the text shown below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "k9lxLeuS2mGU"
      },
      "source": [
        "```\n",
        "LexToken(CLASS,'class',2,14)\n",
        "LexToken(ID,'VariableTest',2,20)\n",
        "LexToken(LBRACE,'{',2,33)\n",
        "LexToken(INT,'int',3,39)\n",
        "LexToken(ID,'j',3,43)\n",
        "LexToken(ASSIGN,'=',3,45)\n",
        "LexToken(INT_LITERAL,'3',3,47)\n",
        "LexToken(SEMI,';',3,48)\n",
        "LexToken(PUBLIC,'public',4,54)\n",
        "LexToken(STATIC,'static',4,61)\n",
        "LexToken(VOID,'void',4,68)\n",
        "LexToken(MAIN,'main',4,73)\n",
        "LexToken(LPAREN,'(',4,77)\n",
        "LexToken(STRING,'String',4,78)\n",
        "LexToken(LBRACKET,'[',4,84)\n",
        "LexToken(RBRACKET,']',4,85)\n",
        "LexToken(ID,'args',4,87)\n",
        "LexToken(RPAREN,')',4,91)\n",
        "LexToken(LBRACE,'{',4,93)\n",
        "LexToken(INT,'int',5,103)\n",
        "LexToken(ID,'i',5,107)\n",
        "LexToken(ASSIGN,'=',5,109)\n",
        "LexToken(THIS,'this',5,111)\n",
        "LexToken(DOT,'.',5,115)\n",
        "LexToken(ID,'j',5,116)\n",
        "LexToken(SEMI,';',5,117)\n",
        "LexToken(INT,'int',6,127)\n",
        "LexToken(ID,'k',6,131)\n",
        "LexToken(ASSIGN,'=',6,133)\n",
        "LexToken(INT_LITERAL,'3',6,135)\n",
        "LexToken(SEMI,';',6,136)\n",
        "LexToken(INT,'int',7,146)\n",
        "LexToken(ID,'p',7,150)\n",
        "LexToken(ASSIGN,'=',7,152)\n",
        "LexToken(INT_LITERAL,'2',7,154)\n",
        "LexToken(TIMES,'*',7,156)\n",
        "LexToken(THIS,'this',7,158)\n",
        "LexToken(DOT,'.',7,162)\n",
        "LexToken(ID,'j',7,163)\n",
        "LexToken(SEMI,';',7,164)\n",
        "LexToken(ASSERT,'assert',9,175)\n",
        "LexToken(ID,'p',9,182)\n",
        "LexToken(EQ,'==',9,184)\n",
        "LexToken(INT_LITERAL,'2',9,187)\n",
        "LexToken(TIMES,'*',9,189)\n",
        "LexToken(ID,'i',9,191)\n",
        "LexToken(SEMI,';',9,192)\n",
        "LexToken(RBRACE,'}',10,198)\n",
        "LexToken(RBRACE,'}',11,200)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is an example of accessing a global function using an object:\n",
        "\n",
        "\n",
        "\n",
        "```java\n",
        "class ObjExample {\n",
        "    int n = 3;\n",
        "\n",
        "    public int doubleMe(int x) {\n",
        "        return x * x;\n",
        "    }\n",
        "\n",
        "    public static void main(String[] args) {\n",
        "        Main obj = new Main();\n",
        "        int v = obj.n;\n",
        "        v = obj.doubleMe(v);\n",
        "        assert v == obj.n * obj.n;\n",
        "    }\n",
        "}\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "IjmjOx_R6s2u"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEQBfNcA2mGV"
      },
      "source": [
        "Carefully study the output of the lexer and make sure that it makes sense. Once you are\n",
        "reasonably happy with the output, try running some of the more tricky tests designed to stress test various corner cases. The repository provided as a base to implement the project contains a large set of tests to verify your code: check them to see more examples.\n",
        "\n",
        "Here is another example:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "b_2NyfyD2mGV"
      },
      "source": [
        "\n",
        "```cpp\n",
        "class ArrayTest {\n",
        "    public static void main(String[] args) {\n",
        "        int[] v = {1, 3, 5, 7, 9};\n",
        "        assert v[3] == 7;\n",
        "    }\n",
        "}\n",
        "```\n",
        "\n",
        "```\n",
        "LexToken(CLASS,'class',1,0)\n",
        "LexToken(ID,'ArrayTest',1,6)\n",
        "LexToken(LBRACE,'{',1,16)\n",
        "LexToken(PUBLIC,'public',2,22)\n",
        "LexToken(STATIC,'static',2,29)\n",
        "LexToken(VOID,'void',2,36)\n",
        "LexToken(MAIN,'main',2,41)\n",
        "LexToken(LPAREN,'(',2,45)\n",
        "LexToken(STRING,'String',2,46)\n",
        "LexToken(LBRACKET,'[',2,52)\n",
        "LexToken(RBRACKET,']',2,53)\n",
        "LexToken(ID,'args',2,55)\n",
        "LexToken(RPAREN,')',2,59)\n",
        "LexToken(LBRACE,'{',2,61)\n",
        "LexToken(INT,'int',3,71)\n",
        "LexToken(LBRACKET,'[',3,74)\n",
        "LexToken(RBRACKET,']',3,75)\n",
        "LexToken(ID,'v',3,77)\n",
        "LexToken(ASSIGN,'=',3,79)\n",
        "LexToken(LBRACE,'{',3,81)\n",
        "LexToken(INT_LITERAL,'1',3,82)\n",
        "LexToken(COMMA,',',3,83)\n",
        "LexToken(INT_LITERAL,'3',3,85)\n",
        "LexToken(COMMA,',',3,86)\n",
        "LexToken(INT_LITERAL,'5',3,88)\n",
        "LexToken(COMMA,',',3,89)\n",
        "LexToken(INT_LITERAL,'7',3,91)\n",
        "LexToken(COMMA,',',3,92)\n",
        "LexToken(INT_LITERAL,'9',3,94)\n",
        "LexToken(RBRACE,'}',3,95)\n",
        "LexToken(SEMI,';',3,96)\n",
        "LexToken(ASSERT,'assert',5,131)\n",
        "LexToken(ID,'v',5,138)\n",
        "LexToken(LBRACKET,'[',5,139)\n",
        "LexToken(INT_LITERAL,'3',5,140)\n",
        "LexToken(RBRACKET,']',5,141)\n",
        "LexToken(EQ,'==',5,143)\n",
        "LexToken(INT_LITERAL,'7',5,146)\n",
        "LexToken(SEMI,';',5,147)\n",
        "LexToken(RBRACE,'}',6,185)\n",
        "LexToken(RBRACE,'}',7,187)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pu0sNqS12mGW"
      },
      "source": [
        "The full list of tokens used by MiniJava is\n",
        "\n",
        "```python\n",
        "    # Set of token names.\n",
        "    tokens = {\n",
        "        # Reserved Keywords\n",
        "        \"CLASS\",\n",
        "        \"EXTENDS\",\n",
        "        \"PUBLIC\",\n",
        "        \"STATIC\",\n",
        "        \"VOID\",\n",
        "        \"MAIN\",\n",
        "        \"STRING\",\n",
        "        \"BOOLEAN\",\n",
        "        \"CHAR\",\n",
        "        \"INT\",\n",
        "        \"IF\",\n",
        "        \"ELSE\",\n",
        "        \"WHILE\",\n",
        "        \"FOR\",\n",
        "        \"ASSERT\",\n",
        "        \"BREAK\",\n",
        "        \"RETURN\",\n",
        "        \"NEW\",\n",
        "        \"THIS\",\n",
        "        \"TRUE\",\n",
        "        \"FALSE\",\n",
        "        \"LENGTH\",\n",
        "        \"PRINT\",\n",
        "        # Literals\n",
        "        \"ID\",\n",
        "        \"INT_LITERAL\",\n",
        "        \"CHAR_LITERAL\",\n",
        "        \"STRING_LITERAL\",\n",
        "        # Operators\n",
        "        \"EQ\",\n",
        "        \"NE\",\n",
        "        \"LE\",\n",
        "        \"GE\",\n",
        "        \"AND\",\n",
        "        \"OR\",\n",
        "        \"ASSIGN\",\n",
        "        \"LT\",\n",
        "        \"GT\",\n",
        "        \"PLUS\",\n",
        "        \"MINUS\",\n",
        "        \"TIMES\",\n",
        "        \"DIVIDE\",\n",
        "        \"MOD\",\n",
        "        \"NOT\",\n",
        "        # Punctuation\n",
        "        \"DOT\",\n",
        "        \"SEMI\",\n",
        "        \"COMMA\",\n",
        "        \"LPAREN\",\n",
        "        \"RPAREN\",\n",
        "        \"LBRACKET\",\n",
        "        \"RBRACKET\",\n",
        "        \"LBRACE\",\n",
        "        \"RBRACE\",\n",
        "    }\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "@webio": {
      "lastCommId": null,
      "lastKernelId": null
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}